{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de9e65f5-b6e4-43b9-814e-4d16006e0bfd",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background-color: white;\n",
    "    padding: 40px 40px;\n",
    "    border-radius: 12px;\n",
    "    box-shadow: 0 4px 12px rgba(255, 0, 0, 0.1);\n",
    "    text-align: center;\n",
    "    font-family: Arial, sans-serif;\n",
    "    margin-top: 30px;\n",
    "\">\n",
    "    <h1 style=\"\n",
    "        color: #000000;\n",
    "        font-size: 45px;\n",
    "        font-weight: bold;\n",
    "        margin: 0;\n",
    "    \">\n",
    "        Customer Input Data Cleaning Pipeline\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd2b4c-2ff8-469a-a4c5-2c99df1eaded",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b93beec5-5275-4538-9c22-759d6de6784e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import html\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c2ce3-369c-4c36-9ef1-235857d1d713",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4034506a-e814-40d7-8b17-b2feb79b9892",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"D:\\crypto tweets dataset\\crypto-query-tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55f32f42-bf65-4d2e-898d-76c2413058bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_time                 0\n",
       "username                  0\n",
       "user_location          4858\n",
       "user_description        992\n",
       "verified                  0\n",
       "followers_count           0\n",
       "following_count           0\n",
       "tweet_like_count          0\n",
       "tweet_retweet_count       0\n",
       "tweet_reply_count         0\n",
       "tweet_quote_count         0\n",
       "text                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb93137-94c7-4e84-89e8-be8c92ef306e",
   "metadata": {},
   "source": [
    "## Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cb60070-4bfa-472a-b00c-d597a3d02752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "def remove_mentions(text):\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    return re.sub(r'#', '', text)\n",
    "\n",
    "def remove_html_entities(text):\n",
    "    return html.unescape(text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "def expand_contractions(text):\n",
    "    contractions = {\n",
    "        \"can't\": \"cannot\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\"\n",
    "    }\n",
    "    for k, v in contractions.items():\n",
    "        text = re.sub(rf\"\\b{k}\\b\", v, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def normalize_repeated_chars(text):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "def remove_non_alpha(text):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in STOP_WORDS]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def clean_tweet(text):\n",
    "    try:\n",
    "        word_count_before = len(text.split())\n",
    "\n",
    "        text = remove_urls(text)\n",
    "        text = remove_mentions(text)\n",
    "        text = remove_hashtags(text)\n",
    "        text = remove_html_entities(text)\n",
    "        text = remove_emojis(text)\n",
    "        text = expand_contractions(text)\n",
    "        text = normalize_repeated_chars(text)\n",
    "        text = remove_non_alpha(text)\n",
    "        text = text.lower()\n",
    "        text = tokenize_and_remove_stopwords(text)\n",
    "\n",
    "        word_count_after = len(text.split())\n",
    "\n",
    "        return text, word_count_before, word_count_after, True\n",
    "\n",
    "    except Exception:\n",
    "        return \"\", 0, 0, False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23728eba-f4ba-43c8-a8a1-cdf807b641dc",
   "metadata": {},
   "source": [
    "## Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43b51f57-8f92-4792-b12e-db1d28bd4061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_crypto_tweets(input_path, output_path):\n",
    "    data = pd.read_csv(input_path)\n",
    "\n",
    "    if 'text' not in data.columns:\n",
    "        raise ValueError(\"Dataset must contain 'text' column\")\n",
    "\n",
    "    results = data['text'].apply(clean_tweet)\n",
    "\n",
    "    data['cleaned_text'] = results.apply(lambda x: x[0])\n",
    "    data['words_before'] = results.apply(lambda x: x[1])\n",
    "    data['words_after'] = results.apply(lambda x: x[2])\n",
    "    data['cleaning_success'] = results.apply(lambda x: x[3])\n",
    "\n",
    "    data.to_csv(output_path, index=False)\n",
    "    print(\"Crypto tweets cleaned successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bbdac56-94f1-4283-a6a6-f74f5f7d1fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crypto tweets cleaned successfully!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    INPUT_FILE = r\"D:\\crypto tweets dataset\\crypto-query-tweets.csv\"\n",
    "    OUTPUT_FILE = r\"D:\\crypto tweets dataset\\cleaned_crypto_tweets.csv\"\n",
    "\n",
    "    process_crypto_tweets(INPUT_FILE, OUTPUT_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
